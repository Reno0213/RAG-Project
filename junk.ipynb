{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maxInt = sys.maxsize\n",
    "# while True:\n",
    "#     # decrease the maxInt value by factor 10 \n",
    "#     # as long as the OverflowError occurs.\n",
    "#     try:\n",
    "#         csv.field_size_limit(maxInt)\n",
    "#         break\n",
    "#     except OverflowError:\n",
    "#         maxInt = int(maxInt/10)\n",
    "\n",
    "\n",
    "chunk_size = 300\n",
    "\n",
    "csv_file_path = 'articles2.csv'\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "# def process_chunk(chunk):\n",
    "#     id = 1\n",
    "\n",
    "#     for i, line in chunk.iterrows():\n",
    "#         if i==0:\n",
    "#             continue\n",
    "#         documents.append(df.iloc[9])\n",
    "#         metadatas.append\n",
    "#         ({\n",
    "#             \"id\": df.iloc[1],\n",
    "#             \"title\": df.iloc[2],\n",
    "#             \"publication\": df.iloc[3],\n",
    "#             \"author\": df.iloc[4],\n",
    "#             \"date\": df.iloc[5],\n",
    "#             \"year\": df.iloc[6],\n",
    "#             \"month\": df.iloc[7]\n",
    "#         })\n",
    "#         ids.append(str(id))\n",
    "#         id+=1\n",
    "\n",
    "#         return documents, metadatas, ids\n",
    "\n",
    "\n",
    "# all_documents = []\n",
    "# all_metadatas = []\n",
    "# all_ids = []\n",
    "\n",
    "for chunk in pd.read_csv(csv_file_path, encoding='utf8', chunksize=chunk_size):\n",
    "    documents.extend(chunk.iloc[:, 9:].astype(str).values.flatten())\n",
    "    #metadata_chunk=({\"id\": chunk.iloc[1],\"title\": chunk.iloc[2],\"publication\": chunk.iloc[3],\"author\": chunk.iloc[4],\"date\": chunk.iloc[5],\"year\": chunk.iloc[6],\"month\": chunk.iloc[7]})\n",
    "    #documents.append(chunk.iloc[9].values)\n",
    "    metadata_chunk = chunk.iloc[:, 1:8].to_dict(orient='records')\n",
    "    metadatas.extend(metadata_chunk)\n",
    "    # metadatas.append\n",
    "    # ({\n",
    "    #     \"id\": chunk.iloc[1],\n",
    "    #     \"title\": chunk.iloc[2],\n",
    "    #     \"publication\": chunk.iloc[3],\n",
    "    #     \"author\": chunk.iloc[4],\n",
    "    #     \"date\": chunk.iloc[5],\n",
    "    #     \"year\": chunk.iloc[6],\n",
    "    #     \"month\": chunk.iloc[7]\n",
    "    # })\n",
    "    ids.extend(chunk.index.astype(str))\n",
    "    #id+=1\n",
    "    # documents, metadatas, ids = process_chunk(chunk)\n",
    "\n",
    "    # all_documents.extend(documents)\n",
    "    # all_metadatas.extend(metadatas)\n",
    "    # all_ids.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'articles1.csv'\n",
    "chunk_size = 20\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "chunks=0\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "for chunk in pd.read_csv(csv_file_path, encoding='utf8', chunksize=chunk_size):\n",
    "    \n",
    "    documents.extend(chunk.iloc[:, 9])\n",
    "    metadata_chunk = chunk.iloc[:, 1:8].to_dict(orient='records')\n",
    "    metadatas.extend(metadata_chunk)\n",
    "    ids.extend(chunk.index.astype(str))\n",
    "    upload(documents, metadatas, ids)\n",
    "    chunks+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import csv\n",
    "import sys\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "# Initialize arrays to store data\n",
    "metadatas = []\n",
    "documents = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "csv_file_path = 'articles1.csv'\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "# Open the CSV file\n",
    "with open(csv_file_path, 'r', encoding='utf8') as csv_file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "\n",
    "    # Skip the header if it exists\n",
    "    next(csv_reader, None)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for id_counter, row in enumerate(csv_reader, start = 1):\n",
    "        # Extract columns 1-8 into a dictionary and append to metadatas\n",
    "        metadatas.append({\n",
    "            'id': row[1],\n",
    "            'title': row[2],\n",
    "            'publication': row[3],\n",
    "            'author': row[4],\n",
    "            'date': row[5],\n",
    "            'year': row[6],\n",
    "            'month': row[7],\n",
    "            'url': row[8]\n",
    "            })\n",
    "        ids.append(str(id))\n",
    "        id +=1\n",
    "\n",
    "        # Extract column 9, split by newline, and append to documents\n",
    "        documents.extend(row[9].split('\\n'))\n",
    "        upload(documents, metadatas, ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "df=pd.read_csv('articles1.csv', encoding='utf8')\n",
    "\n",
    "csv_file_path = 'articles1.csv'\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Function to upload chunks to ChromaDB\n",
    "def upload(documents, metadatas, ids):\n",
    "    for i, document in enumerate(documents):\n",
    "        metadata = metadatas[i]\n",
    "        chunk = text_splitter.create_documents([documents], metadatas=[metadata])\n",
    "        for j, sub_chunk in enumerate(chunk):\n",
    "            collection.add({\"chunk_id\": j, \"content\": sub_chunk, \"source\": metadata[\"source\"]})\n",
    "\n",
    "# Read CSV file in chunks\n",
    "for chunk in pd.read_csv(csv_file_path, encoding='utf8', chunksize=chunk_size):\n",
    "    documents = chunk.iloc[:, 9].tolist()\n",
    "    metadata_chunk = chunk.iloc[:, 1:8].to_dict(orient='records')\n",
    "    metadatas = metadata_chunk\n",
    "    ids = chunk.index.astype(str).tolist()\n",
    "    upload(documents, metadatas, ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCasualLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import csv\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "def chunk_section(filename):\n",
    "    id = 1\n",
    "    with open(filename, 'r', encoding='utf8') as csvfile:\n",
    "        datareader = csv.DictReader(csvfile)\n",
    "        for row in datareader:\n",
    "            #chunk \"content\" row by sentence\n",
    "            lore = row[\"Lore\"]\n",
    "            sentences = sent_tokenize(lore)\n",
    "            for sentence in sentences:\n",
    "                #remove \"content\" from metadata dicitonary\n",
    "                meta = {key: value for key, value in row.items() if key != 'Lore'}\n",
    "                documents.append(sentence)\n",
    "                metadatas.append(meta)\n",
    "                ids.append(str(id))\n",
    "                upload([sentence], [meta], [str(id)])\n",
    "                id +=1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
