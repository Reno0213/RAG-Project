{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maxInt = sys.maxsize\n",
    "# while True:\n",
    "#     # decrease the maxInt value by factor 10 \n",
    "#     # as long as the OverflowError occurs.\n",
    "#     try:\n",
    "#         csv.field_size_limit(maxInt)\n",
    "#         break\n",
    "#     except OverflowError:\n",
    "#         maxInt = int(maxInt/10)\n",
    "\n",
    "\n",
    "chunk_size = 300\n",
    "\n",
    "csv_file_path = 'articles2.csv'\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "# def process_chunk(chunk):\n",
    "#     id = 1\n",
    "\n",
    "#     for i, line in chunk.iterrows():\n",
    "#         if i==0:\n",
    "#             continue\n",
    "#         documents.append(df.iloc[9])\n",
    "#         metadatas.append\n",
    "#         ({\n",
    "#             \"id\": df.iloc[1],\n",
    "#             \"title\": df.iloc[2],\n",
    "#             \"publication\": df.iloc[3],\n",
    "#             \"author\": df.iloc[4],\n",
    "#             \"date\": df.iloc[5],\n",
    "#             \"year\": df.iloc[6],\n",
    "#             \"month\": df.iloc[7]\n",
    "#         })\n",
    "#         ids.append(str(id))\n",
    "#         id+=1\n",
    "\n",
    "#         return documents, metadatas, ids\n",
    "\n",
    "\n",
    "# all_documents = []\n",
    "# all_metadatas = []\n",
    "# all_ids = []\n",
    "\n",
    "for chunk in pd.read_csv(csv_file_path, encoding='utf8', chunksize=chunk_size):\n",
    "    documents.extend(chunk.iloc[:, 9:].astype(str).values.flatten())\n",
    "    #metadata_chunk=({\"id\": chunk.iloc[1],\"title\": chunk.iloc[2],\"publication\": chunk.iloc[3],\"author\": chunk.iloc[4],\"date\": chunk.iloc[5],\"year\": chunk.iloc[6],\"month\": chunk.iloc[7]})\n",
    "    #documents.append(chunk.iloc[9].values)\n",
    "    metadata_chunk = chunk.iloc[:, 1:8].to_dict(orient='records')\n",
    "    metadatas.extend(metadata_chunk)\n",
    "    # metadatas.append\n",
    "    # ({\n",
    "    #     \"id\": chunk.iloc[1],\n",
    "    #     \"title\": chunk.iloc[2],\n",
    "    #     \"publication\": chunk.iloc[3],\n",
    "    #     \"author\": chunk.iloc[4],\n",
    "    #     \"date\": chunk.iloc[5],\n",
    "    #     \"year\": chunk.iloc[6],\n",
    "    #     \"month\": chunk.iloc[7]\n",
    "    # })\n",
    "    ids.extend(chunk.index.astype(str))\n",
    "    #id+=1\n",
    "    # documents, metadatas, ids = process_chunk(chunk)\n",
    "\n",
    "    # all_documents.extend(documents)\n",
    "    # all_metadatas.extend(metadatas)\n",
    "    # all_ids.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'articles1.csv'\n",
    "chunk_size = 20\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "chunks=0\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "for chunk in pd.read_csv(csv_file_path, encoding='utf8', chunksize=chunk_size):\n",
    "    \n",
    "    documents.extend(chunk.iloc[:, 9])\n",
    "    metadata_chunk = chunk.iloc[:, 1:8].to_dict(orient='records')\n",
    "    metadatas.extend(metadata_chunk)\n",
    "    ids.extend(chunk.index.astype(str))\n",
    "    upload(documents, metadatas, ids)\n",
    "    chunks+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import csv\n",
    "import sys\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "# Initialize arrays to store data\n",
    "metadatas = []\n",
    "documents = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "csv_file_path = 'articles1.csv'\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "# Open the CSV file\n",
    "with open(csv_file_path, 'r', encoding='utf8') as csv_file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "\n",
    "    # Skip the header if it exists\n",
    "    next(csv_reader, None)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for id_counter, row in enumerate(csv_reader, start = 1):\n",
    "        # Extract columns 1-8 into a dictionary and append to metadatas\n",
    "        metadatas.append({\n",
    "            'id': row[1],\n",
    "            'title': row[2],\n",
    "            'publication': row[3],\n",
    "            'author': row[4],\n",
    "            'date': row[5],\n",
    "            'year': row[6],\n",
    "            'month': row[7],\n",
    "            'url': row[8]\n",
    "            })\n",
    "        ids.append(str(id))\n",
    "        id +=1\n",
    "\n",
    "        # Extract column 9, split by newline, and append to documents\n",
    "        documents.extend(row[9].split('\\n'))\n",
    "        upload(documents, metadatas, ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "df=pd.read_csv('articles1.csv', encoding='utf8')\n",
    "\n",
    "csv_file_path = 'articles1.csv'\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Function to upload chunks to ChromaDB\n",
    "def upload(documents, metadatas, ids):\n",
    "    for i, document in enumerate(documents):\n",
    "        metadata = metadatas[i]\n",
    "        chunk = text_splitter.create_documents([documents], metadatas=[metadata])\n",
    "        for j, sub_chunk in enumerate(chunk):\n",
    "            collection.add({\"chunk_id\": j, \"content\": sub_chunk, \"source\": metadata[\"source\"]})\n",
    "\n",
    "# Read CSV file in chunks\n",
    "for chunk in pd.read_csv(csv_file_path, encoding='utf8', chunksize=chunk_size):\n",
    "    documents = chunk.iloc[:, 9].tolist()\n",
    "    metadata_chunk = chunk.iloc[:, 1:8].to_dict(orient='records')\n",
    "    metadatas = metadata_chunk\n",
    "    ids = chunk.index.astype(str).tolist()\n",
    "    upload(documents, metadatas, ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCasualLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import csv\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "def upload (documents, metadatas, ids):\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "def chunk_section(filename):\n",
    "    id = 1\n",
    "    with open(filename, 'r', encoding='utf8') as csvfile:\n",
    "        datareader = csv.DictReader(csvfile)\n",
    "        for row in datareader:\n",
    "            #chunk \"content\" row by sentence\n",
    "            lore = row[\"Lore\"]\n",
    "            sentences = sent_tokenize(lore)\n",
    "            for sentence in sentences:\n",
    "                #remove \"content\" from metadata dicitonary\n",
    "                meta = {key: value for key, value in row.items() if key != 'Lore'}\n",
    "                documents.append(sentence)\n",
    "                metadatas.append(meta)\n",
    "                ids.append(str(id))\n",
    "                upload([sentence], [meta], [str(id)])\n",
    "                id +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CHUNKING LOGIC\n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import csv\n",
    "\n",
    "with open('league_lore.csv', 'r', encoding = 'utf8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    championdata = [row for row in csv_reader]\n",
    "    \n",
    "chunk_size = 300\n",
    "chunk_overlap = 20\n",
    "#chunks_result = chunk_section({\"text\": csv_reader, \"source\": \"your_source\"}, chunk_size, chunk_overlap)\n",
    "\n",
    "# def chunk_section(section, chunk_size, chunk_overlap):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         length_function=len)\n",
    "    \n",
    "#     #repeat this function for each {key:value} in list of dictionaries (loop through list of dictionaries)\n",
    "#     #at each iteration, append the new lore + metadata to the lists\n",
    "#     chunks = text_splitter.create_documents(\n",
    "#         #Find a way to retrieve the lore and metadata columns from the csv. sample_section is a dictionary. The code below is creating a list that contains the value associated with the key in dictionary sample_section\n",
    "#         texts=[championdata[\"Lore\"]], \n",
    "#         metadatas=[{\"Champion\": championdata[\"Champion\"], \"Region\": championdata[\"Region\"], \"Link\": championdata[\"Link\"]}])\n",
    "\n",
    "#     return [{\"Lore\": chunk.page_content, \"Champion\": chunk.metadata[\"Champion\"], \"Region\": chunk.metadata[\"Region\"], \"Link\": chunk.metadata[\"Link\"]} for chunk in chunks]\n",
    "\n",
    "def chunk_section(championdata, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks_list = []\n",
    "    for champion_entry in championdata:\n",
    "        chunks = text_splitter.create_documents(\n",
    "            texts=[champion_entry[\"Lore\"]],\n",
    "            metadatas=[{\"Link\": champion_entry[\"Link\"], \"Champion\": champion_entry[\"Champion\"], \"Region\": champion_entry[\"Region\"]}]\n",
    "        )\n",
    "        chunks_list.extend([{\"Link\": champion_entry[\"Link\"], \"Lore\": chunk.page_content, \"Champion\": chunk.metadata[\"Champion\"], \"Region\": chunk.metadata[\"Region\"]} for chunk in chunks])\n",
    "    \n",
    "    return chunks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALE CHUNKING\n",
    "from functools import partial\n",
    "\n",
    "chunks_cd = championdata.flat_map(partial(chunk_section, chunk_size = chunk_size, chunk_overlap = chunk_overlap))\n",
    "print(f\"{chunks_cd.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST CHUNKING SIZE AND OVERLAP\n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import csv\n",
    "\n",
    "# Define the chunk size and overlap\n",
    "chunk_size = 300\n",
    "chunk_overlap = 20\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('league_lore.csv', 'r', encoding='utf8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    championdata = [row for row in csv_reader]\n",
    "\n",
    "# Call the chunk_section method\n",
    "chunks_result = chunk_section(championdata, chunk_size, chunk_overlap)\n",
    "\n",
    "# Print the resulting chunks\n",
    "for chunk in chunks_result:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Embedding"
    ]
   },
   "outputs": [],
   "source": [
    "#EMBEDDING \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from ray.data import ActorPoolStrategy\n",
    "\n",
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            self.embedding_model = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={\"device\": \"cuda\"},\n",
    "                encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100})\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_section(\"league_lore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Which champions are from Shurima\"],\n",
    "    n_results=1,\n",
    "    include=['documents']\n",
    ")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
